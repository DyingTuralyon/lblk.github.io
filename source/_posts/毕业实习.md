---
title: 毕业实习
date: 2021-04-09 13:29:14
tags:
mathjax: True
---
内容取自[【Python网络爬虫与信息提取】北京理工大学](https://www.bilibili.com/video/BV1R7411v74a)
博客原文[blog](https://lblk.github.io/2021/04/09/毕业实习/)
# 第一单元 request库

## 1.1request库的七个方法
增、删、查、取
put delete head get
### requests.request()
基方法
```python
xxx() = request('xxx',url,params = params,**kwargs)
```
URL格式：https://host[:port][path]
### requests.get()
获取html界面

```python
#完整形式
requests.get(url,params = None,**kwargs)
#url:获取页面的url链接
#params:url中的额外参数，字典/字节流格式
#**kwargs:12个控制访问参数
#enum'xxx'{'GET','HEAD','POST','PUT','PATCH','delete','OPTIONS'}
```


### requests.head()
获取html.head
### requests.post()
向网页提交post
### requests.put()
向网页提交put
### requests.patch()
向网页提交局部修改
### requests.delete()
向网页提交删除请求
### 控制访问参数**kwargs
#### params:
```python
kv = {'key1': 'value1','key2': 'value2'}
r = requests.request('GET',url,params=kv)
print(r.url)
```
#### data:

#### Json:

#### headers:

典型：'user-agent': 'chrome/10'

#### Cookies:

#### Auth:

#### files:

```python
fs = {'file': open('data.xls','rb')}
r = requests.post(url,files=fs)
```
#### timeout:
#### proxies:
字典类型，设定代理服务器
```python
pxs = { 'http': 'http://user:pass@10.10.10.1:1234'
				'https': 'https://10.10.10.1:4321'
}
r= requests.request('GET',url,proxies = pxs)
```
#### allow_redirects: 
True/False，默认为True，重定向开关
#### stream
True/False，默认为True，获取内容是否立即下载
#### verify
True/False，默认为True，认证SSL证书
#### cert
保存本地SSL证书的路径

## 1.2request库的返回对象

### response对象的五个属性

#### r.status_code
```python
r.raise_for_status()#如果status_code不是200,引发HTTPError异常
```
返回状态码

#### r.text

网页的二进制页面解析为string形式输出

#### r.encoding

r.text的编码方式

有ISO-8859-1等。。。

#### r.apparent_encoding

request库通过网页内容猜的编码方式，往往比r.encoding更精确，所以总有

```python
r.encoding = r.apparent_encoding
```

#### r.content

HTTP相应的二进制形式

## 1.3Requests库的六种异常

### requests.ConnectionError
网络错误，如DNS查询失败、拒绝连接
### requests.HTTPError
HTTP错误异常
### requests.URLRequired
URL缺失异常
### requests.TooManyRedirects
超过最大重定向次数
### requests.ConnectTimeout
连接服务器超时异常
### requests.Timeout
请求URL超时，产生超时异常
###小模版
来自[Python __name__](https://www.runoob.com/note/39287)
```python
#通用request代码框架
import requests
def getHTMLText(url):
	try:
		r = requests.get(url,timeout = 30)
		r = raise_for_status()
		r.encoding = r.apparent_encoding
		return r.text
	except:
		return “产生异常”
if __name__ = "__main__"
		url = "https://www.baidu.com"
		print(getHTMLText(url))
```

# 第二单元 使用爬虫的限制

人类行为不受协议限制

## Robos协议

地址：~/robots.txt

人类行为不受协议限制

自动化&高频次行为受协议限制

## robot_parser

```python
from urllib.robotparser import RobotFileParser
# 创建RobotFileParser对象
rp = RobotFileParser()
# 然后通过set_url()方法设置了robots.txt的链接。
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
#can_fetch()方法判断了网页是否可以被抓取。
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
```

以上截自[爬虫分析Robots协议](https://blog.csdn.net/chengqiuming/article/details/86319506)



# 第三单元 简单模版

# 第四单元 BS4库

```
from bs4 import BeautifulSoup
soup = BeautifulSoup(r.text,'html.parser')
soup = BeautifulSoup(open('./index.html'),'html.parser')
print(soup.prettify())
```

## bs4解析器

### 'html.parser'

### 'lxml'

### 'xml'

### 'html5lib'

## bs类的基本元素

### 标签Tag

soup.<tag>

### 标签名

<tag>.name

### 标签属性

<tag>.attrs['*']

### 非属性的字符串

<tag>.string

ps:该方法会无视标签中各种标签输出所有字符

### 注释

<!-- *  -->

## html的基本格式

## 标签树的遍历

### 上行遍历

#### .parent

父节点

#### .parents

迭代父类型

### 下行遍历

#### .contents

子节点的列表

#### .children

子节点的迭代类型

#### .descondants

所有子孙节点的迭代类型

### 平行遍历

平行遍历的下一个节点不一定是标签类型，也可能是NavigableStringg6

#### .next_sibling

平行下一个标签

#### .previous_sibling

平行上一个标签

#### .next_siblings

接下来所有标签的迭代类型

#### .previous_siblings

之前所有标签的迭代类型

# 第五单元 信息标记和信息提取

## 三种信息标记格式

### XML

```html
<>...</>
<img src = **>...</img>
<img src = ** />
<!-- xxx -->
```



### JSON

```json
"key" : "value"
"key" : ["value1","value2"]
"key" : {
  "key1" : "value1"
  "key2" : "value2"
}
```



### YAML

```YAML
name : NCEPU
name :
	-name1
	-name2
name: |
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
name :
	key1:value1
	key2:value2
#yaml可以写注释
```

### 三种形式的比较

Xml:早，扩展性好，繁杂

用于网上信息传递

Json:信息有类型，适合程序处理，比较简洁，无注释

用于作为程序的接口

Yaml:信息无类型，最简洁，可读性最高，有注

用于作为系统的配置文件

## 信息提取的方法

### 完全解析

### 搜索关键词

### 融合方法

实例：

提取所有url

（1）搜索<a>

```
for link in soup.find_all('a')
```



（2）完全解析所有的<a>

```
link.get('href')
```

```
<>.find_all(name,attrs,recursive,string,**kwargs)
#name可以是列表
#name传入True表示所有名字
#attrs限定是否存在某属性值或限定某属性的属性值
#recursive bool型，默认true，即搜索所有子孙


<tag>(..)#等价于<tag>.find_all(..)
soup(..)#等价于  soup.find_all(..)
```



### CSS Selector

```css
<HTML>.css('a::attr(href)').extract()
#a -> name of Tag
#href -> name of attr of a
```



# 第六单元 解析网页实例

# 第七单元 正则表达式与Re库

通俗地来说
正则表达式是用
一个字符串来表达一个字符串的集合G的方法
当出现匹配的需求时，我们将待匹配的集合F与正则表达式所表示的集合取交集，就得到了匹配结果，当然，这是纯数学的描述，详细算法中不可能枚举所有的G中的元素

## 常用操作符

### .

任意某个字符

### []

字符的取值范围，如[a-z]

### [ˆ]

字符的排除范围，如[ˆabc]

### *

前1字符[$0-\infty$]次贪婪匹配

### +

前1字符[$1-\infty$]贪婪匹配

### ?

前1字符[$0-1$]次贪婪匹配

### |

与（）配合可连用

### {m}

前1字符m次匹配

### {m,n}

前1字符[]$m-n$]次贪婪匹配

### ˆ

匹配字符串开头

### $

匹配字符串结尾

### ()

分组标记

### \d

匹配某个数字

### \w

大小写字母或数字或下划线

## 定义一个正则表达式ztext

### raw string

### string

r'ztext'

## re库的主要功能函数

### re.search(pattern, string,flags=0)

匹配出一个，返回match

### Re.match()

从开始位置起，自带ˆ，返回match

### Re.findall()

返回列表类型

### re.split()

按匹配的结果分割，返回子串的列表类型

可以用maxsplit控制分组

### Re.finditer()

返回匹配结果的迭代类型

### re.sub(pattern,repl,string,count = 0,flags=0)

替换所有匹配，返回字符串

Repl：替换用字符串

Count：匹配的最大替换次数，默认0意味着全替换。

## match的使用

```python
match = re.search(...)
if match:
	match.group(0)
```

### .string

原文

### .re

pattern

### .pos

检索的位置

### .endpos

检索的位置



### 方法.group(0)

返回匹配到的串

### 方法.start()

返回匹配点的起点

### 方法.end()

返回匹配点的终点（str规范中的终点在最后一个字符的后一个位置，这里同理）

### 方法.span()
返回（.start(),.end())二元组


## re.compile的使用

```python
#regex = re.compile(pattern,flags=0)
pat = re.compile(r'101')#pattern类型
rst = pat.search('10101')

```

## 最小匹配操作符

在贪婪操作符后加？，如：'*?'、'+?'

# 第\w八单元 高级实例

# 第九单元 scrapy框架

