---
title: 毕业实习
date: 2021-04-09 13:29:14
tags:
mathjax: True
---
内容取自[【Python网络爬虫与信息提取】北京理工大学](https://www.bilibili.com/video/BV1R7411v74a)
博客原文[blog](https://lblk.github.io/2021/04/09/毕业实习/)
# 第一单元 request库

## 1.1request库的七个方法
增、删、改、查
put delete post get
|方法名|作用|



### requests.request()
基方法
```python
xxx() = request('xxx',url,params = params,**kwargs)
```
URL格式：https://host[:port][path]
```python
#完整形式
requests.get(url,params = None,**kwargs)
#url:获取页面的url链接
#params:url中的额外参数，字典/字节流格式
#**kwargs:12个控制访问参数
#enum'xxx'{'GET','HEAD','POST','PUT','PATCH','delete','OPTIONS'}
```
|----|----|
|requests.get()   |获取html界面|
|requests.head()|获取html.head |
|requests.post()  |向网页提交post |
|requests.put()   |向网页提交put |
|requests.patch()  |向网页提交局部修改 |
|requests.delete() |向网页提交删除请求|

### 控制访问参数**kwargs
#### params:
```python
kv = {'key1': 'value1','key2': 'value2'}
r = requests.request('GET',url,params=kv)
print(r.url)
```
#### data:

#### Json:

#### headers:

典型：'user-agent': 'chrome/10'

#### Cookies:

#### Auth:

#### files:

```python
fs = {'file': open('data.xls','rb')}
r = requests.post(url,files=fs)
```
#### timeout:
#### proxies:
字典类型，设定代理服务器
```python
pxs = { 'http': 'http://user:pass@10.10.10.1:1234'
				'https': 'https://10.10.10.1:4321'
}
r= requests.request('GET',url,proxies = pxs)
```
#### allow_redirects: 
True/False，默认为True，重定向开关
#### stream
True/False，默认为True，获取内容是否立即下载
#### verify
True/False，默认为True，认证SSL证书
#### cert
保存本地SSL证书的路径
### **kwargs总结
|控制访问参数总表|用法|
|----|----|
|params|r = requests.request('GET',url,params={'key1': 'value1','key2': 'value2'})|
|data|
|json|
|headers|'user-agent': 'chrome/10'|
|Cookies|
|Auth|
|files|fs = {'file': open('data.xls','rb')};r = requests.post(url,files=fs)|
|timeout|timeout = 30|
|proxies|r= requests.request('GET',url,proxies = pxs)|
|allow_redirects|
|stream|
|verify|
|cert| |
## 1.2request库的返回对象

### response对象的五个属性
|response.属性|作用|
|----| ----|
|r.status_code|返回状态码|
|r.text|网页的二进制页面解析为string形式输出|
|r.encoding|r.text的编码方式,有ISO-8859-1等。。。|
|r.apparent_encoding|request库通过网页内容猜的编码方式，往往比r.encoding更精确|
|r.content|HTTP相应的二进制形式|

### response对象的两个常见方法
|response.方法|作用|
|r.raise_for_status()|如果status_code不是200,引发HTTPError异常|
|r.encoding = r.apparent_encoding|用解析得到的编码方式更正默认编码方式|



## 1.3Requests库的六种异常

|异常|原因|
|----|----|
| requests.ConnectionError|网络错误，如DNS查询失败、拒绝连接|
|requests.HTTPError|HTTP错误异常|
|requests.URLRequired|URL缺失异常|
| requests.TooManyRedirects|超过最大重定向次数|
|requests.ConnectTimeout|连接服务器超时异常|
|requests.Timeout|请求URL超时，产生超时异常|







### 小模版
来自[Python __name__](https://www.runoob.com/note/39287)
```python
#通用request代码框架
import requests
def getHTMLText(url):
	try:
		r = requests.get(url,timeout = 30)
		r = raise_for_status()
		r.encoding = r.apparent_encoding
		return r.text
	except:
		return “产生异常”
if __name__ = "__main__"
		url = "https://www.baidu.com"
		print(getHTMLText(url))
```

# 第二单元 使用爬虫的限制

人类行为不受协议限制

## Robos协议

地址：~/robots.txt

人类行为不受协议限制

自动化&高频次行为受协议限制

## robot_parser

```python
from urllib.robotparser import RobotFileParser
# 创建RobotFileParser对象
rp = RobotFileParser()
# 然后通过set_url()方法设置了robots.txt的链接。
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
#can_fetch()方法判断了网页是否可以被抓取。
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
```

以上截自[爬虫分析Robots协议](https://blog.csdn.net/chengqiuming/article/details/86319506)



# 第三单元 简单模版

# 第四单元 BS4库

```
from bs4 import BeautifulSoup
soup = BeautifulSoup(r.text,'html.parser')
soup = BeautifulSoup(open('./index.html'),'html.parser')
print(soup.prettify())
```

## bs4解析器
|解析器名|
|---|
|'html.parser'|
|'lxml'|
|'xml'|
|'html5lib'|

## bs类的基本元素



|引用标签的数据|代码|备注|
|---|---|---|
|引用整个标签|soup.<tag>|
|标签名|<tag>.name|
|标签属性|<tag>.attrs['*']|
|非属性的字符串|<tag>.string|该方法会无视标签中的标签输出一切字符|




### 注释

<!-- *  -->

## html的基本格式
```
<html>
├── <head>
│  		 └──<title>
└── <body>
  		 ├── <p>
       │    └── <b>
       └── <p>
        		├── <a>
        		└── <a>
```


## 标签树的遍历

### 上行遍历

| 方法    | 作用 |
| ------- | ---- |
| .parent |   父节点   |
|.parents|迭代父类型|

### 下行遍历

| 方法    | 作用 |
| ------- | ---- |
|.contents|子节点的列表|
|.children|子节点的迭代类型|
|.descondants|所有子孙节点的迭代类型|






### 平行遍历

| 方法    | 作用 |
| ------- | ---- |
|.next_sibling|平行下一个标签|
|.previous_sibling|平行上一个标签|
|.next_siblings|接下来所有标签的迭代类型|
|.previous_siblings|之前所有标签的迭代类型|


# 第五单元 信息标记和信息提取

## 三种信息标记格式

### XML

```html
<>...</>
<img src = **>...</img>
<img src = ** />
<!-- xxx -->
```



### JSON

```json
"key" : "value"
"key" : ["value1","value2"]
"key" : {
  "key1" : "value1"
  "key2" : "value2"
}
```



### YAML

```YAML
name : NCEPU
name :
	-name1
	-name2
name: |
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
name :
	key1:value1
	key2:value2
#yaml可以写注释
```

### 三种形式的比较
|形式|特征|用途|
|---|---|---|
|Xml|早，扩展性好，繁杂|用于网上信息传递|
|Json|信息有类型，适合程序处理，比较简洁，无注释|用于作为程序的接口|
|Yaml|信息无类型，最简洁，可读性最高，有注|用于作为系统的配置文件|

## 信息提取的方法

### 完全解析

### 搜索关键词

### 融合方法

实例：

提取所有url

（1）搜索<a>

```
for link in soup.find_all('a')
```



（2）完全解析所有的<a>

```
link.get('href')
```

```
<>.find_all(name,attrs,recursive,string,**kwargs)
#name可以是列表
#name传入True表示所有名字
#attrs限定是否存在某属性值或限定某属性的属性值
#recursive bool型，默认true，即搜索所有子孙


<tag>(..)#等价于<tag>.find_all(..)
soup(..)#等价于  soup.find_all(..)
```



### CSS Selector

```css
<HTML>.css('a::attr(href)').extract()
#a -> name of Tag
#href -> name of attr of a 
```



# 第六单元 解析网页实例

# 第七单元 正则表达式与Re库

通俗地来说
正则表达式是用
一个字符串来表达一个字符串的集合G的方法
当出现匹配的需求时，我们将待匹配的集合F与正则表达式所表示的集合取交集，就得到了匹配结果，当然，这是纯数学的描述，详细算法中不可能枚举所有的G中的元素

## 常用操作符

### .

任意某个字符

### []

字符的取值范围，如[a-z]

### [ˆ]

字符的排除范围，如[ˆabc]

### *

前1字符[$0-\infty$]次贪婪匹配

### +

前1字符[$1-\infty$]贪婪匹配

### ?

前1字符[$0-1$]次贪婪匹配

### |

与（）配合可连用

### {m}

前1字符m次匹配

### {m,n}

前1字符[]$m-n$]次贪婪匹配

### ˆ

匹配字符串开头

### $

匹配字符串结尾

### ()

分组标记

### \d

匹配某个数字

### \w

大小写字母或数字或下划线

## 定义一个正则表达式ztext

### raw string

### string

r'ztext'

## re库的主要功能函数

### re.search(pattern, string,flags=0)

匹配出一个，返回match

### Re.match()

从开始位置起，自带ˆ，返回match

### Re.findall()

返回列表类型

### re.split()

按匹配的结果分割，返回子串的列表类型

可以用maxsplit控制分组

### Re.finditer()

返回匹配结果的迭代类型

### re.sub(pattern,repl,string,count = 0,flags=0)

替换所有匹配，返回字符串

Repl：替换用字符串

Count：匹配的最大替换次数，默认0意味着全替换。

## match的使用

```python
match = re.search(...)
if match:
	match.group(0)
```
|属性|值|
|---|---|
|.string|原文|
|.re|pattern|
|.pos|检索的位置起点|
|.endpos|检索的位置终点|




|方法|值|
|---|---|
|.group(0)|返回匹配到的串|
|.start()|返回匹配点的起点|
|.end()|返回匹配点的终点（str规范中的终点在最后一个字符的后一个位置，这里同理）|
|.span()|返回（.start(),.end())二元组|


## re.compile的使用

```python
#regex = re.compile(pattern,flags=0)
pat = re.compile(r'101')#pattern类型
rst = pat.search('10101')

```

## 最小匹配操作符

在贪婪操作符后加？，如：'*?'、'+?'

# 第\w八单元 高级实例

[朴素爬虫爬取豆瓣top250](https://lblk.github.io/2021/04/12/douban-top250/)

# 第九单元 scrapy框架

