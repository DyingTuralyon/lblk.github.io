---
title: 毕业实习
date: 2021-04-09 13:29:14
tags:
---
内容取自[【Python网络爬虫与信息提取】北京理工大学](https://www.bilibili.com/video/BV1R7411v74a)
# 第一单元 request库

## 1.1request库的七个方法
增、删、查、取
put delete head get
### requests.request()
基方法
```python
xxx() = request('xxx',url,params = params,**kwargs)
```
URL格式：https://host[:port][path]
### requests.get()
获取html界面

```python
#完整形式
requests.get(url,params = None,**kwargs)
#url:获取页面的url链接
#params:url中的额外参数，字典/字节流格式
#**kwargs:12个控制访问参数
#enum'xxx'{'GET','HEAD','POST','PUT','PATCH','delete','OPTIONS'}
```


### requests.head()
获取html.head
### requests.post()
向网页提交post
### requests.put()
向网页提交put
### requests.patch()
向网页提交局部修改
### requests.delete()
向网页提交删除请求
### 控制访问参数**kwargs
#### params:
```python
kv = {'key1': 'value1','key2': 'value2'}
r = requests.request('GET',url,params=kv)
print(r.url)
```
#### data:

#### Json:

#### headers:

典型：'user-agent': 'chrome/10'

#### Cookies:

#### Auth:

#### files:

```python
fs = {'file': open('data.xls','rb')}
r = requests.post(url,files=fs)
```
#### timeout:
#### proxies:
字典类型，设定代理服务器
```python
pxs = { 'http': 'http://user:pass@10.10.10.1:1234'
				'https': 'https://10.10.10.1:4321'
}
r= requests.request('GET',url,proxies = pxs)
```
#### allow_redirects: 
True/False，默认为True，重定向开关
#### stream
True/False，默认为True，获取内容是否立即下载
#### verify
True/False，默认为True，认证SSL证书
#### cert
保存本地SSL证书的路径

## 1.2request库的返回对象

### response对象的五个属性

#### r.status_code
```python
r.raise_for_status()#如果status_code不是200,引发HTTPError异常
```
返回状态码

#### r.text

网页的二进制页面解析为string形式输出

#### r.encoding

r.text的编码方式

有ISO-8859-1等。。。

#### r.apparent_encoding

request库通过网页内容猜的编码方式，往往比r.encoding更精确，所以总有

```python
r.encoding = r.apparent_encoding
```

#### r.content

HTTP相应的二进制形式

## 1.3Requests库的六种异常

### requests.ConnectionError
网络错误，如DNS查询失败、拒绝连接
### requests.HTTPError
HTTP错误异常
### requests.URLRequired
URL缺失异常
### requests.TooManyRedirects
超过最大重定向次数
### requests.ConnectTimeout
连接服务器超时异常
### requests.Timeout
请求URL超时，产生超时异常
###小模版
来自[Python __name__](https://www.runoob.com/note/39287)
```python
#通用request代码框架
import requests
def getHTMLText(url):
	try:
		r = requests.get(url,timeout = 30)
		r = raise_for_status()
		r.encoding = r.apparent_encoding
		return r.text
	except:
		return “产生异常”
if __name__ = "__main__"
		url = "https://www.baidu.com"
		print(getHTMLText(url))
```

# 第二单元 使用爬虫的限制

人类行为不受协议限制

## Robos协议

地址：~/robots.txt

人类行为不受协议限制

自动化&高频次行为受协议限制

## robot_parser

```python
from urllib.robotparser import RobotFileParser
# 创建RobotFileParser对象
rp = RobotFileParser()
# 然后通过set_url()方法设置了robots.txt的链接。
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
#can_fetch()方法判断了网页是否可以被抓取。
print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
```

以上截自[爬虫分析Robots协议](https://blog.csdn.net/chengqiuming/article/details/86319506)



# 第三单元 简单模版

